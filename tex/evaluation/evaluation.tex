\documentclass{standalone}

\begin{document}
	It is now time to evaluate our implemented application by performing a critical analysis to help determine the extent of our application's fulfilment to the initial project goals. This shall be done by performing a series of both objective and subjective tests aiming to expose its strengths and weaknesses.

	\section{Unit Tests}
		Unit testing is arguably the most reliable software development technique for the purposes of scrutinizing an application in order to prove its operational correctness. A unit is the smallest testable part of an application, such as an individual function. A unit test is a short code fragment intended to test a single unit of an application.

		During the development process, an application will evolve as modifications are introduced. This is normal behaviour, although the process has a habit of leaving behind bugs. Having a complete suite of unit tests enable the developer to automatically identify bugs and ensure that their code retains its previous validity. Unit tests can be implemented in a number of ways. As the meat of our application is programmed in JavaScript, we will be using Jest \parencite{jest} - one of the many available JavaScript testing solutions. Jest also offers specialised support for React via snapshot testing. Various modules within our application contain unit tests in a subdirectory named \enquote{\texttt{\_\_tests\_\_}}. These tests have been developed according to both the black-box and white-box testing methods, to best guarantee the completeness of our application. \footnote{Due to the time-sensitive nature of the project, the current implemented unit tests do not yet have 100\% coverage of our application.}

		\begin{figure}[!htbp]
			\begin{formal}
				\verbatiminput{unittestoutput.tex}
			\end{formal}
			\caption{Capture of the console output produced from executing our Jest unit tests.} \label{fig:unitTests}
		\end{figure}

		As seen from \autoref{fig:unitTests}, we have unit tests covering several modules of our application. Should a new bug be discovered within one of these modules, a new test would be created capable of detecting said bug. For information regarding what is actually being tested, we encourage the reader to refer to the JavaScript test file's source-code; they are well documented.

	\section{Game Performance}
		The game has been developed up to a playable state, yet there is still interest in seeing just how much is capable before it begins to breakdown. To gain a stronger understanding of just what our game is capable of, we conduct experiments measuring the performance impact caused by various game mechanics. For each of these experiments, the goal is derive a sensible evaluation based upon an accumulation of quantifiable data.\footnote{A separate Git branch was created for the purposes of recording benchmark data. It can be found on our GitHub repository (see \fullref{sec:softwareDevelopmentProcess}).}

		\subsection{Tick Update Performance}
			The game loop is responsible for drawing the graphics and, more importantly, updating the game state. It is absolutely critical that the update function's call duration does not exceed the duration of its associated tick. If this were the case, our game's tick-rate would begin to fall - leading to undesired game behaviour. Hence we perform a series of experiments measuring the implemented update function's execution duration under various scenarios. These scenarios will vary by two controllable factors: the total number of connected players\footnote{All connected players, besides the host player, are computer players controlled by artificial intelligence. This means our results better resemble those of worst-case performance.}, and the elapsed time since the beginning of the game round. Each experiment scenario is repeated five times to help mitigate erroneous results.

			\thispagestyle{empty}
				\subimport{resources/performance/}{graph.tex}

			As can be seen in \autoref{fig:gamePerformanceChart}, the results indicate there is definitely a significant correlation between our game's performance and the two examined factors. It can be seen that an increase to either factor will result in our game state update duration being prolonged. Not only this, but performance is subject to the combination of either factor. The reasoning for this is quite simple, and our collision detection is the primary cause: in general, our collision detection is required to perform more extensive searches if our game arena consists of a greater number of objects. Furthermore, as the round naturally progresses in time, each player will be travelling around the game arena; contributing additional objects. Hence, round progression produces objects proportionally to the number of alive players; further burdening the task of collision detection. However with this being said, even in the worst case, our game state updates occur at an extremely fast rate - well within the required bounds of once every 15 milliseconds (see \fullref{sec:gameMechanics}).

	\section{Capability of Artificial Intelligence}
		Play, and record the outcome, of numerous games against the AI player.

	\section{User Feedback}
		The implemented game could be objectively perfect, however if users believe that the project fails to hold promise, then the investigated ambition of the project would conclude as a failure - as the designed techniques are not compatible with modern web-browsers. Therefore, it is worthwhile to collect a sample of user opinions, enabling us to form a concrete understanding of where our application stands. Hence, we introduce 5TODO users to play a few games, then document their opinions by filling in a short survey (see \fullref{fig:userSurvey}).
\end{document}